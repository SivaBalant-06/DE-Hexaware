{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOBcPLF12I7lfMYt4WyJADe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","log_data = pd.read_csv(\"/content/sample_data/Marks_data.csv\")\n","print(log_data)\n","type(log_data)"],"metadata":{"id":"cnJQs_YXPX-V"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JnOw9hMzNOBy"},"outputs":[],"source":["import pyspark"]},{"cell_type":"code","source":["from pyspark import SparkContext\n","from pyspark.sql import SparkSession\n","\n","sc = SparkContext.getOrCreate()\n","#create a sparksession\n","spark = SparkSession.builder.appName('PySpark DataFrame From RDD').getOrCreate()"],"metadata":{"id":"1AY59WKNNpzc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#create a rdd\n","rdd = sc.parallelize([('C',85,76,87,91), ('B',85,76,87,91), (\"A\", 85,78,96,92), (\"A\", 92,76,89,96)], 4)\n","print(type(rdd))"],"metadata":{"id":"4hUQWPhcOJkm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Converting the RDD into PySpark DataFrame\n","#schema rdd,rdd,dataframe\n","sub = ['Division','English','Mathematics','Physics','Chemistry']\n","marks_df= spark.createDataFrame(rdd, schema=sub)\n","print(type(marks_df))"],"metadata":{"id":"MGq8Cd6BO0GN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["marks_df.printSchema()\n","marks_df.show()"],"metadata":{"id":"Lu676ytKPPCh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName(\"FileRead\").getOrCreate()\n","\n","# Read Text File\n","txt_df = spark.read.text(\"/example.txt\")\n","txt_df.printSchema()\n","\n","\n","# Read CSV File\n","csv_df = spark.read.option(\"header\", True).csv(\"/sample.csv\")\n","csv_df.printSchema()\n","\n","\n","# Read JSON File\n","json_df = spark.read.json(\"/sample.json\")\n","json_df.printSchema()"],"metadata":{"id":"H6CprUHE3NDd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = spark.createDataFrame(\n","[\n","(\"sue\", 32),\n","(\"li\", 3),\n","(\"bob\", 75),\n","(\"heo\", 13),\n","],\n","[\"first_name\", \"age\"],\n",")\n","\n","df.show()"],"metadata":{"id":"TasdYiBF8wAL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import col, when\n","df1 = df.withColumn( \"life_stage\",\n","when(col(\"age\") < 13, \"child\")\n",".when(col(\"age\").between(13, 19), \"teenager\")\n",".otherwise(\"adult\"),)\n","df1.show()"],"metadata":{"id":"Jin6oawA9UDU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","spark = SparkSession \\\n",".builder \\\n",".appName(\"Python Spark create RDD example\") \\\n",".config(\"spark.some.config.option\", \"some-value\") \\\n",".getOrCreate()\n","Employee = spark.createDataFrame([\n","(\"1\", 'Joe', '70000' , '1'),\n","(\"2\", 'Henry', '80000' , '2'),\n","(\"3\", 'Sam', '60000' , '2'),\n","(\"4\", 'Max', '90000', '1')],\n","['Id', 'Name', 'Sallary','DepartmentId']\n",")\n","Employee.show()"],"metadata":{"id":"LAC2Nc6T-ltI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pyspark\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"practice-Hexaware\").getOrCreate()\n","df = spark.read.csv(\"/content/sample_data/Marks_data.csv\")\n","df.show() #to show the entire file"],"metadata":{"id":"adRZBNlo_5Eo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# importing module\n","import pyspark\n","\n","# import lit function\n","from pyspark.sql.functions import concat_ws, lit\n","\n","# importing sparksession from pyspark.sql module\n","from pyspark.sql import SparkSession\n","\n","# creating sparksession and giving an app name\n","spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n","\n","# list of employee data\n","data = [[\"1\", \"sravan\", \"company 1\"],\n","[\"2\", \"ojaswi\", \"company 1\"],\n","[\"3\", \"rohith\", \"company 2\"],\n","[\"4\", \"sridevi\", \"company 1\"],\n","[\"5\", \"bobby\", \"company 1\"]]\n","\n","# specify column names\n","columns = ['ID', 'NAME', 'Company']\n","# creating a dataframe from the lists of data\n","df = spark.createDataFrame(data, columns)\n","\n","df.withColumn(\"salary\",lit(30000)).show()\n","\n","df.withColumn(\"Details\", concat_ws(\"-\", \"NAME\", 'Company')).show()"],"metadata":{"id":"hFuJVm4CCdX1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.withColumn(\"salary\", df.ID*2300).show()"],"metadata":{"id":"jtYUhLXfKkKS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if 'salary' not in df.columns:\n","  df.withColumn(\"salary\", lit(34000)).show()"],"metadata":{"id":"0g8tRi0lK61a"},"execution_count":null,"outputs":[]}]}