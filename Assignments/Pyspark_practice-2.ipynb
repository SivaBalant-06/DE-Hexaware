{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNqZ9LQl0+CZ0OVva8KQ5BH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"6bVFAjCItyOZ"},"outputs":[],"source":["import pyspark\n","\n","from pyspark import SparkContext\n","from pyspark.sql import SparkSession\n","\n","sc = SparkContext.getOrCreate()\n","#create a sparksession\n","spark = SparkSession.builder.appName('PySpark DataFrame From RDD').getOrCreate()"]},{"cell_type":"code","source":["# collect() action\n","collect_rdd = sc.parallelize([1,2,3,4,5])\n","print(collect_rdd.collect())"],"metadata":{"id":"jYPImOq7udk0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# count() action\n","count_rdd = sc.parallelize([1,2,3,4,5,5,6,7,8,9])\n","print(count_rdd.count()) #count the list"],"metadata":{"id":"vCjdSKOCuiXQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# first() action\n","first_rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n","print(first_rdd.first()) #returns the first element"],"metadata":{"id":"fdsd9eEyuvv5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# take() action\n","take_rdd = sc.parallelize([1,2,3,4,5])\n","print(take_rdd.take(3)) #extract the elements"],"metadata":{"id":"RU2tH9kRvGQj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# reduce() action\n","reduce_rdd = sc.parallelize([1,3,4,6])\n","print(reduce_rdd.reduce(lambda x, y : x + y)) #returns the sum of all elements"],"metadata":{"id":"RMsA7krAvSS8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#saveAsTextFile() action\n","save_rdd = sc.parallelize([1,2,3,4,5,6])\n","save_rdd.saveAsTextFile('file.txt')"],"metadata":{"id":"meQqJd8wvlvs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pyspark\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, lit\n","from pyspark.sql.types import StructType, StructField, StringType,IntegerType\n","\n","spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n","\n","data = [('James','','Smith','1991-04-01','M',3000),\n","  ('Michael','Rose','','2000-05-19','M',4000),\n","  ('Robert','','Williams','1978-09-05','M',4000),\n","  ('Maria','Anne','Jones','1967-12-01','F',4000),\n","  ('Jen','Mary','Brown','1980-02-17','F',-1)\n","]\n","columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n","df = spark.createDataFrame(data=data, schema = columns)\n","df.printSchema()\n","df.show(truncate=False)"],"metadata":{"id":"jJOL4BPJF6Jc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df2 = df.withColumn(\"salary\",col(\"salary\").cast(\"Integer\"))\n","df2.printSchema()\n","df2.show(truncate=False)"],"metadata":{"id":"2X4a9s-QRCaJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df3 = df.withColumn(\"salary\",col(\"salary\")*100)\n","df3.printSchema()\n","df3.show(truncate=False)"],"metadata":{"id":"VRkA0WX3RzdV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df4 = df.withColumn(\"CopiedColumn\",col(\"salary\")* -1)\n","df4.printSchema()"],"metadata":{"id":"f3S-klpuSGBG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df5 = df.withColumn(\"Country\", lit(\"USA\"))\n","df5.printSchema()\n","df5.show(truncate=False)"],"metadata":{"id":"ggSOsyxzSfHh"},"execution_count":null,"outputs":[]}]}